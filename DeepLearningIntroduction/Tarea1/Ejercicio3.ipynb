{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Ejercicio3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPfAI9JTnjYaTkMb0wMRlI8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JonathanMartignon/DeepLearningIntroduction/blob/main/Tarea1/Ejercicio3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dA6fgYkHG3iP"
      },
      "source": [
        "## Entrena una red completamente conectada para aproximar la compuerta XOR2\n",
        "\n",
        "Por: \n",
        "- Martiñón Luna Jonathan José"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vzaU9-_cG-w8"
      },
      "source": [
        "Recordamos que la compuerta XOR ($\\oplus$) es:\n",
        "\n",
        "| $x_1$ | $x_2$ | $y$ |\n",
        "|-------|-------|-----|\n",
        "| 0     | 0     | 0   |\n",
        "| 0     | 1     | 1   |\n",
        "| 1     | 0     | 1   |\n",
        "| 1     | 1     | 0   |\n",
        "\n",
        "\n",
        "Y que es posible aproximar este tipo combinando múltiples LTU conectadas en red. Por ejemplo, es posible llevar a cabo la operación XOR con operaciones OR, AND y NAND en la siguiente ecuación:\n",
        "\n",
        "$$\n",
        "x_1 \\mathbin{\\oplus} x_2 = (x_1 \\lor x_2) \\land \\neg(x_1 \\land x_2)\n",
        "$$\n",
        "\n",
        "Recordando las compuertas OR:\n",
        "\n",
        "| $x_1$ | $x_2$ | $y$ |\n",
        "|-------|-------|-----|\n",
        "| 0     | 0     | 0   |\n",
        "| 0     | 1     | 1   |\n",
        "| 1     | 0     | 1   |\n",
        "| 1     | 1     | 1   |\n",
        "\n",
        "y and:\n",
        "\n",
        "| $x_1$ | $x_2$ | $y$ |\n",
        "|-------|-------|-----|\n",
        "| 0     | 0     | 0   |\n",
        "| 0     | 1     | 0   |\n",
        "| 1     | 0     | 0   |\n",
        "| 1     | 1     | 1   |\n",
        "\n",
        "Por lo que $\\neg(x_1 \\land x_2)$\n",
        "\n",
        "| $x_1$ | $x_2$ | $y$ |\n",
        "|-------|-------|-----|\n",
        "| 0     | 0     | 1   |\n",
        "| 0     | 1     | 1   |\n",
        "| 1     | 0     | 1   |\n",
        "| 1     | 1     | 0   |\n",
        "\n",
        "Finalmente obteniendo para $x_1 \\mathbin{\\oplus} x_2 = (x_1 \\lor x_2) \\land \\neg(x_1 \\land x_2)$:\n",
        "\n",
        "| or | neg and | $y$ |\n",
        "|----|---------|-----|\n",
        "| 0  |   1     | 0   |\n",
        "| 1  |   1     | 1   |\n",
        "| 1  |   1     | 1   |\n",
        "| 1  |   0     | 0   |\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jfU8xgfZITG1"
      },
      "source": [
        "---\n",
        "\n",
        "# Librerías\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rXLpk8WfGWCG"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "import numpy as np"
      ],
      "execution_count": 578,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qGbfiIBWJUHJ"
      },
      "source": [
        "----\n",
        "\n",
        "# Definición de la Arquitectura\n",
        "\n",
        "----"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WAfmdWcI7AJZ"
      },
      "source": [
        "Creo que me confundí un poco, y a lo mejor no resultó en la mejor implementación. Estuve buscando y encontré que algunos usuarios realizaban clases a parte y después las agregaban, por lo que pensé en hacer lo mismo.\n",
        "\n",
        "Claro está, que probablemente exista un sobreajuste en tanto que los resultados los devuelve directamente en 1 o 0 (No sé si se valía)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z2vhuGBGIbxC"
      },
      "source": [
        "#-----------------------------\n",
        "# Clase para compuerta or\n",
        "#-----------------------------\n",
        "\n",
        "class Step(nn.Module):\n",
        "  def __init__(self):\n",
        "    # inicilización del objeto padre, obligatorio\n",
        "    super(Step, self).__init__()\n",
        "\n",
        "  def forward(self,x):\n",
        "      self.result = torch.zeros(x.shape[0],1)\n",
        "      self.m = x.shape[0]\n",
        "\n",
        "      for i in range(0,self.m):\n",
        "          self.result[i] = x[i,0] + x[i,1]\n",
        "\n",
        "      return self.result\n",
        "#-----------------------------\n",
        "# Clase para compuerta and\n",
        "#-----------------------------\n",
        "\n",
        "class Nand(nn.Module):\n",
        "  def __init__(self):\n",
        "    # inicilización del objeto padre, obligatorio\n",
        "    super(Nand, self).__init__()\n",
        "\n",
        "  def forward(self,x):\n",
        "      self.result = torch.zeros(x.shape[0],1)\n",
        "      self.m = x.shape[0]\n",
        "\n",
        "      for i in range(0,self.m):\n",
        "          self.result[i] = x[i,0] * x[i,1]\n",
        "\n",
        "      return self.result\n",
        "\n",
        "#-----------------------------\n",
        "# Clase para la red\n",
        "#-----------------------------\n",
        "\n",
        "class FCN(nn.Module):\n",
        "    \n",
        "    # inicializador\n",
        "    def __init__(self):\n",
        "        \n",
        "        # inicilización del objeto padre, obligatorio\n",
        "        super(FCN, self).__init__()\n",
        "        \n",
        "        # tamaño de las capas\n",
        "        self.I = 2 * 4\n",
        "\n",
        "        #--------------------\n",
        "        # definición de capas\n",
        "        #--------------------\n",
        "        # La primera es una capa de la compuerta Or \n",
        "        self.first = nn.Sequential(\n",
        "            Step(),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        \n",
        "        #La segunda será para la compuerta AND\n",
        "\n",
        "        self.second = Nand()\n",
        "        self.third = nn.ReLU()\n",
        "      \n",
        "\n",
        "        #La última es para la compuerta XOR\n",
        "\n",
        "        self.fourth = Nand()\n",
        "      \n",
        "\n",
        "\n",
        "    # método para inferencia\n",
        "    def forward(self, x):\n",
        "\n",
        "      self.x = x\n",
        "      self.orx = self.first(self.x)\n",
        "      self.andx =  self.second(self.x)\n",
        "      self.Nand = torch.logical_not(self.andx,out=torch.empty(self.andx.shape[0], dtype=torch.int16)).reshape(self.andx.shape[0],-1)\n",
        "\n",
        "      self.xor = self.fourth(torch.cat((self.orx.view([4, 1]),self.Nand.view([4, 1])), dim=1))\n",
        "        \n",
        "      return self.xor"
      ],
      "execution_count": 579,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fXEVMqmTLUsL",
        "outputId": "00eb5920-abd9-430b-82df-ea748060e036"
      },
      "source": [
        "model1 = FCN()\n",
        "print(model1)"
      ],
      "execution_count": 580,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "FCN(\n",
            "  (first): Sequential(\n",
            "    (0): Step()\n",
            "    (1): ReLU()\n",
            "  )\n",
            "  (second): Nand()\n",
            "  (third): ReLU()\n",
            "  (fourth): Nand()\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a8Gsl8ff5Jyd",
        "outputId": "4629eecf-fa4f-4f2c-e190-04793ae43c40"
      },
      "source": [
        "x = torch.from_numpy(np.array([[0.,0.],\n",
        "                               [0.,1.],\n",
        "                               [1.,0.],\n",
        "                               [1.,1.]]))\n",
        "y = model1(x)\n",
        "print('-----------------------------')\n",
        "print('x_1 \\tx_2 \\ty')\n",
        "print('-----------------------------')\n",
        "for i in range(y.shape[0]):\n",
        "    print(f'{x[i,0]}\\t{x[i,1]}\\t{y[i][0]}')"
      ],
      "execution_count": 581,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-----------------------------\n",
            "x_1 \tx_2 \ty\n",
            "-----------------------------\n",
            "0.0\t0.0\t0.0\n",
            "0.0\t1.0\t1.0\n",
            "1.0\t0.0\t1.0\n",
            "1.0\t1.0\t0.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:78: UserWarning: An output with one or more elements was resized since it had shape [4], which does not match the required output shape [4, 1].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /pytorch/aten/src/ATen/native/Resize.cpp:19.)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FEsNcCCn7YU0"
      },
      "source": [
        "---\n",
        "\n",
        "# Método 2\n",
        "\n",
        "---\n",
        "\n",
        "Estuve investigando un poco y, aprovechándome del inciso en el que se podía utilizar cualquier herramienta de pytorch, encontré precisamente algunos que eran para compuertas lógicas.\n",
        "\n",
        "Claro que viéndome **aún más flojo**, y reitero, no sé si era válido o el objetivo, encontré la función directa (de pytorch)\n",
        "\n",
        "Aunque no estoy seguro si cuenta, porque creo que no hay capas."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aho4ff0T7pui"
      },
      "source": [
        "class Metodo2(nn.Module):\n",
        "    \n",
        "    # inicializador\n",
        "    def __init__(self):\n",
        "        # inicilización del objeto padre, obligatorio\n",
        "        super(Metodo2, self).__init__()\n",
        "\n",
        "\n",
        "    # método para inferencia\n",
        "    def forward(self, x):\n",
        "      self.x = x\n",
        "      self.xor = torch.logical_xor(self.x[:,0],self.x[:,1],\n",
        "                                   out=torch.empty(self.x.shape[0], dtype=torch.int16))\n",
        "      return self.xor"
      ],
      "execution_count": 582,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ru2LOaob8PEr",
        "outputId": "0c0debd8-7ae5-47e4-910a-92d2527237f5"
      },
      "source": [
        "model2 = Metodo2()\n",
        "print(model2)"
      ],
      "execution_count": 583,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Metodo2()\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GX3S4O888HxA",
        "outputId": "f8af0fa8-fed5-416c-f556-3f9085db29e8"
      },
      "source": [
        "y2 = model2(x)\n",
        "print('-----------------------------')\n",
        "print('x_1 \\tx_2 \\ty')\n",
        "print('-----------------------------')\n",
        "for i in range(y2.shape[0]):\n",
        "    print(f'{x[i,0]}\\t{x[i,1]}\\t{y2[i]}')"
      ],
      "execution_count": 584,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-----------------------------\n",
            "x_1 \tx_2 \ty\n",
            "-----------------------------\n",
            "0.0\t0.0\t0\n",
            "0.0\t1.0\t1\n",
            "1.0\t0.0\t1\n",
            "1.0\t1.0\t0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rDQSVPr7vsO1"
      },
      "source": [
        "# Último método (Basado en la libreta 1e_mnist_fcn)\n",
        "\n",
        "Platicando con algunos compañeros, observo que hacen uso de nn.Linear(), que a mi parecer lo que hace es $\\hat{y}=wx+\\beta$ pero no entiendo bien de dónde salen los $w$ o los $\\beta$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DJqq7GOK3RPa"
      },
      "source": [
        "1) Creamos la arquitectura"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q5dGFwqnxKer"
      },
      "source": [
        "class Last_FCN(nn.Module):\n",
        "    \n",
        "    # inicializador\n",
        "    def __init__(self):\n",
        "        \n",
        "        # inicilización del objeto padre, obligatorio\n",
        "        super(Last_FCN, self).__init__()\n",
        "        \n",
        "        # tamaño de las capas\n",
        "        self.I = 2\n",
        "\n",
        "        #--------------------\n",
        "        # definición de capas\n",
        "        #--------------------\n",
        "\n",
        "        # Existen una entrada de 2 dimensiones\n",
        "        # Y será una salida de 2 dimensiones también\n",
        "        self.first = nn.Sequential(\n",
        "            nn.Linear(2,2),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "        # Existen otra entrada de 2 dimensiones\n",
        "        # Y será una salida de 1 dimensión\n",
        "        self.second = nn.Sequential(\n",
        "            nn.Linear(2,1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    # método para inferencia\n",
        "    def forward(self, x):\n",
        "\n",
        "      self.x = x\n",
        "      self.One = self.first(self.x)\n",
        "      self.Sec = self.second(self.One)\n",
        "        \n",
        "      return self.Sec"
      ],
      "execution_count": 585,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Iucs3Ux3T2O"
      },
      "source": [
        "2) Presentamos el modelo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jDvbpI5r0Tar",
        "outputId": "8ccbbc79-1692-4739-d03a-c7adfddb241e"
      },
      "source": [
        "modelo = Last_FCN()\n",
        "modelo"
      ],
      "execution_count": 586,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Last_FCN(\n",
              "  (first): Sequential(\n",
              "    (0): Linear(in_features=2, out_features=2, bias=True)\n",
              "    (1): Sigmoid()\n",
              "  )\n",
              "  (second): Sequential(\n",
              "    (0): Linear(in_features=2, out_features=1, bias=True)\n",
              "    (1): Sigmoid()\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 586
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TZ16GUB33dzp"
      },
      "source": [
        "3) Hacemos inferencia con datos *de juguete*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vkfJfnor0b7N",
        "outputId": "6c9d5fbe-ba32-443e-c296-a6d6fef9df8f"
      },
      "source": [
        "# Simulando un tensor con dimensiones iguales\n",
        "# a nuestras entradas 01,00,11,10\n",
        "xx = torch.zeros((4,2))\n",
        "\n",
        "# Utilizando la red\n",
        "y = modelo(xx)\n",
        "\n",
        "# Verificando la salida\n",
        "print(f'{x.shape} => {y.shape}')"
      ],
      "execution_count": 587,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([4, 2]) => torch.Size([4, 1])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y9A2LRHi-GI1"
      },
      "source": [
        "4) Procedemos al entrenamiento"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_wozpXsW9Fb2"
      },
      "source": [
        "\n",
        "#----------------------\n",
        "# Entrenamiento\n",
        "#----------------------\n",
        "\n",
        "# Honestamente, dado las pocas combinaciones que podíamos tener, no creí\n",
        "# que fuera necesario un conjunto test. A mi parecer habría un sobre ajuste.\n",
        "# Es por ello que no lo puse\n",
        "\n",
        "def train(model, data, y_true, learning_rate=1e-3, epocas=20, muestra=10):\n",
        "\n",
        "    # historiales\n",
        "    loss_hist = []\n",
        "    \n",
        "    # optimizador por gradiente estocástico descendiente\n",
        "    opt = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    MSE = nn.MSELoss()\n",
        "\n",
        "    # ciclo de entrenamiento por época\n",
        "    for epoch in range(epocas):\n",
        "\n",
        "        # computamos logits\n",
        "        y_lgts = model(data)\n",
        "        \n",
        "        # computamos la pérdida\n",
        "\n",
        "        loss = MSE(y_lgts, y_true)\n",
        "        \n",
        "        # vaciamos los gradientes\n",
        "        opt.zero_grad()\n",
        "        \n",
        "        # retropropagamos\n",
        "        loss.backward()\n",
        "        \n",
        "        # actualizamos parámetros\n",
        "        opt.step()\n",
        "\n",
        "        loss_hist.append(loss)\n",
        "\n",
        "        # imprimimos progreso\n",
        "        if epoch % muestra == 0:\n",
        "          print(f'E{epoch:02} '\n",
        "                f'loss=[{loss}]')\n",
        "\n",
        "    return loss_hist, acc_hist"
      ],
      "execution_count": 588,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k0Z8vTFpLmVC"
      },
      "source": [
        "Probamos con 20 épocas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gHSMsa60HEZ7",
        "outputId": "ea42954e-b21f-401d-a460-36f2b05be895"
      },
      "source": [
        "# instanciamos un modelo\n",
        "modelo = Last_FCN()\n",
        "\n",
        "## Nuestras x\n",
        "data = torch.tensor([[0.,1.],\n",
        "                     [0.,0.],\n",
        "                     [1.,1.],\n",
        "                     [1.,0.]])\n",
        "y_true = torch.tensor([[0.],[1.],[1.],[0.]])\n",
        "# entrenamos\n",
        "loss_hist, acc_hist = train(modelo,data,y_true,learning_rate=0.08)\n",
        "y = modelo(data)\n",
        "print('-----------------------------')\n",
        "print('x_1 \\tx_2 \\ty')\n",
        "print('-----------------------------')\n",
        "for i in range(y.shape[0]):\n",
        "    print(f'{x[i,0]}\\t{x[i,1]}\\t{y[i][0]}')"
      ],
      "execution_count": 589,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "E00 loss=[0.2614307403564453]\n",
            "E10 loss=[0.25016480684280396]\n",
            "-----------------------------\n",
            "x_1 \tx_2 \ty\n",
            "-----------------------------\n",
            "0.0\t0.0\t0.4826590120792389\n",
            "0.0\t1.0\t0.4800490140914917\n",
            "1.0\t0.0\t0.49193698167800903\n",
            "1.0\t1.0\t0.4893735349178314\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K5l4XNe3Lq94"
      },
      "source": [
        "Probamos con 100 épocas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_SpO8LlJHK1a",
        "outputId": "bd4c952d-899a-4748-b351-4208638cd06a"
      },
      "source": [
        "# instanciamos un modelo\n",
        "modelo = Last_FCN()\n",
        "\n",
        "## Nuestras x\n",
        "data = torch.tensor([[0.,1.],\n",
        "                     [0.,0.],\n",
        "                     [1.,1.],\n",
        "                     [1.,0.]])\n",
        "y_true = torch.tensor([[0.],[1.],[1.],[0.]])\n",
        "# entrenamos\n",
        "loss_hist, acc_hist = train(modelo,data,y_true,learning_rate=0.08,epocas=100,muestra=10)\n",
        "y = modelo(data)\n",
        "print('-----------------------------')\n",
        "print('x_1 \\tx_2 \\ty')\n",
        "print('-----------------------------')\n",
        "for i in range(y.shape[0]):\n",
        "    print(f'{x[i,0]}\\t{x[i,1]}\\t{y[i][0]}')"
      ],
      "execution_count": 590,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "E00 loss=[0.2771655321121216]\n",
            "E10 loss=[0.253320574760437]\n",
            "E20 loss=[0.2497926652431488]\n",
            "E30 loss=[0.24504898488521576]\n",
            "E40 loss=[0.2333054393529892]\n",
            "E50 loss=[0.20893244445323944]\n",
            "E60 loss=[0.1868313103914261]\n",
            "E70 loss=[0.17653951048851013]\n",
            "E80 loss=[0.1725013256072998]\n",
            "E90 loss=[0.17071278393268585]\n",
            "-----------------------------\n",
            "x_1 \tx_2 \ty\n",
            "-----------------------------\n",
            "0.0\t0.0\t0.6702086925506592\n",
            "0.0\t1.0\t0.6609236001968384\n",
            "1.0\t0.0\t0.6653938293457031\n",
            "1.0\t1.0\t0.05476263165473938\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BtQTHJ4WMLYI"
      },
      "source": [
        "Probamos con 1000 épocas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fN2DSxgRMKzI",
        "outputId": "7ea1f110-af40-4bde-f157-4a9726ba4793"
      },
      "source": [
        "# instanciamos un modelo\n",
        "modelo = Last_FCN()\n",
        "\n",
        "## Nuestras x\n",
        "data = torch.tensor([[0.,1.],\n",
        "                     [0.,0.],\n",
        "                     [1.,1.],\n",
        "                     [1.,0.]])\n",
        "y_true = torch.tensor([[0.],[1.],[1.],[0.]])\n",
        "# entrenamos\n",
        "loss_hist, acc_hist = train(modelo,data,y_true,epocas=1000,learning_rate=0.08,muestra=100)\n",
        "\n",
        "y = modelo(data)\n",
        "print('-----------------------------')\n",
        "print('x_1 \\tx_2 \\ty')\n",
        "print('-----------------------------')\n",
        "for i in range(y.shape[0]):\n",
        "    print(f'{x[i,0]}\\t{x[i,1]}\\t{y[i][0]}')"
      ],
      "execution_count": 591,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "E00 loss=[0.25084030628204346]\n",
            "E100 loss=[0.005242226645350456]\n",
            "E200 loss=[0.0013153718318790197]\n",
            "E300 loss=[0.0006740197422914207]\n",
            "E400 loss=[0.0004220898263156414]\n",
            "E500 loss=[0.00029351815464906394]\n",
            "E600 loss=[0.00021778541849926114]\n",
            "E700 loss=[0.00016889387916307896]\n",
            "E800 loss=[0.00013524522364605218]\n",
            "E900 loss=[0.00011096531670773402]\n",
            "-----------------------------\n",
            "x_1 \tx_2 \ty\n",
            "-----------------------------\n",
            "0.0\t0.0\t0.009954786859452724\n",
            "0.0\t1.0\t0.9916370511054993\n",
            "1.0\t0.0\t0.9896552562713623\n",
            "1.0\t1.0\t0.009753783233463764\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WiIuAcyxXCsO"
      },
      "source": [
        "Con 10,000 épocas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t-8aWuODUUxN",
        "outputId": "8c536a3f-b5eb-4eb5-d483-58ca7a9d898d"
      },
      "source": [
        "# instanciamos un modelo\n",
        "modelo = Last_FCN()\n",
        "\n",
        "## Nuestras x\n",
        "data = torch.tensor([[0.,1.],\n",
        "                     [0.,0.],\n",
        "                     [1.,1.],\n",
        "                     [1.,0.]])\n",
        "y_true = torch.tensor([[0.],[1.],[1.],[0.]])\n",
        "# entrenamos\n",
        "loss_hist, acc_hist = train(modelo,data,y_true,epocas=10000,learning_rate=0.08,muestra=1000)\n",
        "y = modelo(data)\n",
        "print('-----------------------------')\n",
        "print('x_1 \\tx_2 \\ty')\n",
        "print('-----------------------------')\n",
        "for i in range(y.shape[0]):\n",
        "    print(f'{x[i,0]}\\t{x[i,1]}\\t{y[i][0]}')"
      ],
      "execution_count": 592,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "E00 loss=[0.2849968373775482]\n",
            "E1000 loss=[0.00016201267135329545]\n",
            "E2000 loss=[4.627384987543337e-05]\n",
            "E3000 loss=[2.0212692106724717e-05]\n",
            "E4000 loss=[1.0322917660232633e-05]\n",
            "E5000 loss=[5.682119535777019e-06]\n",
            "E6000 loss=[3.25557402902632e-06]\n",
            "E7000 loss=[1.908321337396046e-06]\n",
            "E8000 loss=[1.1334534519846784e-06]\n",
            "E9000 loss=[6.78610263094015e-07]\n",
            "-----------------------------\n",
            "x_1 \tx_2 \ty\n",
            "-----------------------------\n",
            "0.0\t0.0\t0.00058381212875247\n",
            "0.0\t1.0\t0.9993900060653687\n",
            "1.0\t0.0\t0.9992448091506958\n",
            "1.0\t1.0\t0.0005915439687669277\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n4IJndYUM11X"
      },
      "source": [
        "# Análisis de resultados\n",
        "\n",
        "Todos los modelos se evaluaron en:\n",
        " - 20 épocas\n",
        " - 100 épocas\n",
        " - 1,000 épocas\n",
        " - 10,000 épocas\n",
        "\n",
        "## Utilizando el gradiente descendiente como optimizador y lr = 0.01\n",
        "\n",
        "- Utilizando 2 Funciones de activación RELU\n",
        "\n",
        "  Nuestro mejor modelo fue entrenando 1,000 épocas, con una pérdida de 0.126370787. Y valores predichos de:\n",
        "$$\n",
        "\\hat{y} = [0.49,0.97,0.49,0.04]\n",
        "$$\n",
        "\n",
        "- Utilizando Sigmoide en el primer caso y Relu en el segundo\n",
        "\n",
        "  Nuevamente nuestro mejor valor aparenta ser con 1,000 épocas con una pérdida de 0.2519682049751282 Con 10,000 épocas se pierde. \n",
        "$$\n",
        "\\hat{y} = [0.56,0.50,0.49,0.43]\n",
        "$$\n",
        "\n",
        "- Utilizando RELU en el primer caso y Sigmoide en el segundo\n",
        "\n",
        "  Realmente, en este caso no hay alguno que pueda decir que se trate del mejor, pues no se alcanza a distinguir que los valores medios sean mejores a los límites. En este caso, con 1,000 épocas se pierde totalmente y otorga mismo valor a todas: 0.4837266802787781\n",
        "\n",
        "- Finalmente se usaron 2 Sigmoides\n",
        "Realmente no existían valores muy diferentes a la combinación anterior. También se probó un Lr = 0.1, pero tampoco lograba sobresalir\n",
        "\n",
        "## Utilizando el Adam como optimizador (Lr =0.01):\n",
        "- 2 ReLu:\n",
        "\n",
        "  En definitiva, esta combinación no es nada óptima. Todo lo envió a 0, exepto al entrenar 10,000 épocas, donde envió a 0.5 todos los valores \n",
        "\n",
        "- Sigmoide y Relu:\n",
        "\n",
        "  Mismo problema, todo lo envía a 0 (en este caso a partir de 100 épocas)\n",
        "\n",
        "- 2 Sigmoides:\n",
        "\n",
        "  Casi un aprendizaje perfecto (eso sí, fue hasta las 10,000 épocas). Obteniendo una pérdida de 1.1293\n",
        "$$\n",
        "\\hat{y}=[0.0023,0.99,0.990.0026]\n",
        "$$\n",
        "\n",
        "Al probar con un lr de 0.08, obteníamos resultados bastante cercanos desde las 1,000 épocas, una tasa de pérdida de: 0.000169.\n",
        "\n",
        "Mientras que con 10,000 la tasa de pérdida lo decrementaba a: 0.0000006786\n",
        "\n",
        "\n",
        "## Conclusiones\n",
        "Podemos concluir que el mejor modelo resulta al utilizar 2 sigmoides como funciones de activación y el método de optimización ADAM, con una taza de aprendizaje del 0.08\n",
        "\n",
        "Nota: En el notebook sólo se mostrarán los resultados con el método anteriormente mencionado\n"
      ]
    }
  ]
}